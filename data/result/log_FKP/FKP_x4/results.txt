{'alpha': 1e-06,
 'batch_size': 100,
 'cond_label_size': None,
 'conditional': False,
 'device': device(type='cuda', index=0),
 'evaluate': False,
 'flip_var_order': False,
 'generate': True,
 'hidden_size': 25,
 'input_dims': (1, 19, 19),
 'input_size': 361,
 'kernel_size': 19,
 'log_interval': 500,
 'lr': 0.0001,
 'n_blocks': 5,
 'n_components': 1,
 'n_epochs': 100,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'normalization': 0.16908,
 'output_dir': './result/log_FKP/FKP_x4',
 'restore_file': '',
 'results_file': './result/log_FKP/FKP_x4/results.txt',
 'seed': 0,
 'sf': 4,
 'start_epoch': 0,
 'train': True,
 'val_save_path': './result/datasets/Kernel_validation_set'}
KernelPrior(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=361, out_features=25, bias=True)
        (1): Tanh()
        (2): Linear(in_features=25, out_features=25, bias=True)
        (3): Tanh()
        (4): Linear(in_features=25, out_features=361, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=361, out_features=25, bias=True)
        (1): ReLU()
        (2): Linear(in_features=25, out_features=25, bias=True)
        (3): ReLU()
        (4): Linear(in_features=25, out_features=361, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=361, out_features=25, bias=True)
        (1): Tanh()
        (2): Linear(in_features=25, out_features=25, bias=True)
        (3): Tanh()
        (4): Linear(in_features=25, out_features=361, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=361, out_features=25, bias=True)
        (1): ReLU()
        (2): Linear(in_features=25, out_features=25, bias=True)
        (3): ReLU()
        (4): Linear(in_features=25, out_features=361, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=361, out_features=25, bias=True)
        (1): Tanh()
        (2): Linear(in_features=25, out_features=25, bias=True)
        (3): Tanh()
        (4): Linear(in_features=25, out_features=361, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=361, out_features=25, bias=True)
        (1): ReLU()
        (2): Linear(in_features=25, out_features=25, bias=True)
        (3): ReLU()
        (4): Linear(in_features=25, out_features=361, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=361, out_features=25, bias=True)
        (1): Tanh()
        (2): Linear(in_features=25, out_features=25, bias=True)
        (3): Tanh()
        (4): Linear(in_features=25, out_features=361, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=361, out_features=25, bias=True)
        (1): ReLU()
        (2): Linear(in_features=25, out_features=25, bias=True)
        (3): ReLU()
        (4): Linear(in_features=25, out_features=361, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=361, out_features=25, bias=True)
        (1): Tanh()
        (2): Linear(in_features=25, out_features=25, bias=True)
        (3): Tanh()
        (4): Linear(in_features=25, out_features=361, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=361, out_features=25, bias=True)
        (1): ReLU()
        (2): Linear(in_features=25, out_features=25, bias=True)
        (3): ReLU()
        (4): Linear(in_features=25, out_features=361, bias=True)
      )
    )
    (9): BatchNorm()
  )
)
Evaluate (epoch 0): -logp(x) = 253.552 +/- 2.800
Evaluate (epoch 1): -logp(x) = 171.358 +/- 3.257
Evaluate (epoch 2): -logp(x) = 182.603 +/- 2.774
Evaluate (epoch 3): -logp(x) = 121.230 +/- 3.327
Evaluate (epoch 4): -logp(x) = 102.752 +/- 3.385
Evaluate (epoch 5): -logp(x) = 100.037 +/- 3.303
Evaluate (epoch 6): -logp(x) = 119.173 +/- 2.962
Evaluate (epoch 7): -logp(x) = 84.371 +/- 3.273
Evaluate (epoch 8): -logp(x) = 102.470 +/- 2.996
Evaluate (epoch 9): -logp(x) = 75.923 +/- 3.248
Evaluate (epoch 10): -logp(x) = 81.297 +/- 3.111
Evaluate (epoch 11): -logp(x) = 55.430 +/- 3.349
Evaluate (epoch 12): -logp(x) = 66.353 +/- 3.184
Evaluate (epoch 13): -logp(x) = 65.661 +/- 3.155
Evaluate (epoch 14): -logp(x) = 73.524 +/- 3.075
Evaluate (epoch 15): -logp(x) = 73.516 +/- 3.040
Evaluate (epoch 16): -logp(x) = 73.744 +/- 2.968
Evaluate (epoch 17): -logp(x) = 62.014 +/- 3.064
Evaluate (epoch 18): -logp(x) = 51.755 +/- 3.124
Evaluate (epoch 19): -logp(x) = 50.121 +/- 3.178
Evaluate (epoch 20): -logp(x) = 43.601 +/- 3.181
Evaluate (epoch 21): -logp(x) = 73.567 +/- 2.909
Evaluate (epoch 22): -logp(x) = 39.234 +/- 3.149
Evaluate (epoch 23): -logp(x) = 35.253 +/- 3.205
Evaluate (epoch 24): -logp(x) = 39.953 +/- 3.132
Evaluate (epoch 25): -logp(x) = 31.810 +/- 3.145
Evaluate (epoch 26): -logp(x) = 64.053 +/- 2.949
Evaluate (epoch 27): -logp(x) = 77.909 +/- 2.717
Evaluate (epoch 28): -logp(x) = 43.457 +/- 2.978
Evaluate (epoch 29): -logp(x) = 41.943 +/- 3.024
Evaluate (epoch 30): -logp(x) = 77.108 +/- 2.687
Evaluate (epoch 31): -logp(x) = 64.307 +/- 2.806
Evaluate (epoch 32): -logp(x) = 34.665 +/- 3.075
Evaluate (epoch 33): -logp(x) = 20.530 +/- 3.172
Evaluate (epoch 34): -logp(x) = 75.744 +/- 2.817
Evaluate (epoch 35): -logp(x) = 36.038 +/- 3.017
Evaluate (epoch 36): -logp(x) = 30.075 +/- 3.036
Evaluate (epoch 37): -logp(x) = 28.364 +/- 3.051
Evaluate (epoch 38): -logp(x) = 17.094 +/- 3.112
Evaluate (epoch 39): -logp(x) = 15.353 +/- 3.147
Evaluate (epoch 40): -logp(x) = 37.656 +/- 2.938
Evaluate (epoch 41): -logp(x) = 9.519 +/- 3.173
Evaluate (epoch 42): -logp(x) = 11.578 +/- 3.137
Evaluate (epoch 43): -logp(x) = 15.346 +/- 3.129
Evaluate (epoch 44): -logp(x) = 33.253 +/- 3.029
Evaluate (epoch 45): -logp(x) = 51.463 +/- 2.845
Evaluate (epoch 46): -logp(x) = 28.572 +/- 2.999
Evaluate (epoch 47): -logp(x) = 12.785 +/- 3.109
Evaluate (epoch 48): -logp(x) = 13.258 +/- 3.108
Evaluate (epoch 49): -logp(x) = 7.343 +/- 3.173
Evaluate (epoch 50): -logp(x) = 0.054 +/- 3.201
Evaluate (epoch 51): -logp(x) = -1.214 +/- 3.209
Evaluate (epoch 52): -logp(x) = 8.683 +/- 3.181
Evaluate (epoch 53): -logp(x) = 2.858 +/- 3.170
Evaluate (epoch 54): -logp(x) = 7.231 +/- 3.137
Evaluate (epoch 55): -logp(x) = 2.713 +/- 3.170
Evaluate (epoch 56): -logp(x) = 1.915 +/- 3.171
Evaluate (epoch 57): -logp(x) = 2.448 +/- 3.177
Evaluate (epoch 58): -logp(x) = 1.878 +/- 3.165
Evaluate (epoch 59): -logp(x) = 6.148 +/- 3.123
Evaluate (epoch 60): -logp(x) = -1.582 +/- 3.188
Evaluate (epoch 61): -logp(x) = -0.753 +/- 3.180
Evaluate (epoch 62): -logp(x) = 0.089 +/- 3.183
Evaluate (epoch 63): -logp(x) = 0.275 +/- 3.165
Evaluate (epoch 64): -logp(x) = -0.781 +/- 3.191
Evaluate (epoch 65): -logp(x) = -1.941 +/- 3.184
Evaluate (epoch 66): -logp(x) = -0.382 +/- 3.166
Evaluate (epoch 67): -logp(x) = -4.374 +/- 3.208
Evaluate (epoch 68): -logp(x) = 0.007 +/- 3.161
Evaluate (epoch 69): -logp(x) = 0.079 +/- 3.163
Evaluate (epoch 70): -logp(x) = -5.617 +/- 3.206
Evaluate (epoch 71): -logp(x) = -6.639 +/- 3.216
Evaluate (epoch 72): -logp(x) = -1.988 +/- 3.189
Evaluate (epoch 73): -logp(x) = 1.119 +/- 3.159
Evaluate (epoch 74): -logp(x) = -3.302 +/- 3.191
Evaluate (epoch 75): -logp(x) = -4.209 +/- 3.199
Evaluate (epoch 76): -logp(x) = -3.439 +/- 3.196
Evaluate (epoch 77): -logp(x) = -4.864 +/- 3.191
Evaluate (epoch 78): -logp(x) = -5.556 +/- 3.203
Evaluate (epoch 79): -logp(x) = -7.480 +/- 3.207
Evaluate (epoch 80): -logp(x) = -5.609 +/- 3.202
Evaluate (epoch 81): -logp(x) = -5.480 +/- 3.205
Evaluate (epoch 82): -logp(x) = -6.279 +/- 3.209
Evaluate (epoch 83): -logp(x) = -5.662 +/- 3.200
Evaluate (epoch 84): -logp(x) = -5.440 +/- 3.209
Evaluate (epoch 85): -logp(x) = -5.149 +/- 3.200
Evaluate (epoch 86): -logp(x) = -5.698 +/- 3.199
Evaluate (epoch 87): -logp(x) = -7.695 +/- 3.215
Evaluate (epoch 88): -logp(x) = -6.714 +/- 3.199
Evaluate (epoch 89): -logp(x) = -3.999 +/- 3.194
Evaluate (epoch 90): -logp(x) = -4.560 +/- 3.188
Evaluate (epoch 91): -logp(x) = -6.680 +/- 3.214
Evaluate (epoch 92): -logp(x) = -6.417 +/- 3.204
Evaluate (epoch 93): -logp(x) = -7.125 +/- 3.221
Evaluate (epoch 94): -logp(x) = -7.328 +/- 3.206
Evaluate (epoch 95): -logp(x) = -5.629 +/- 3.198
Evaluate (epoch 96): -logp(x) = -6.984 +/- 3.209
Evaluate (epoch 97): -logp(x) = -5.333 +/- 3.192
Evaluate (epoch 98): -logp(x) = -4.320 +/- 3.211
Evaluate (epoch 99): -logp(x) = -6.095 +/- 3.202
